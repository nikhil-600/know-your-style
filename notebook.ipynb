{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPool2D,BatchNormalization,Activation,Flatten,Dropout\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = sorted(list(paths.list_images(\"imgs/\")))\n",
    "random.seed(40)\n",
    "random.shuffle(imagePaths)\n",
    "train_imagePaths,val_imagePaths=train_test_split(imagePaths,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15386, 1710)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_imagePaths),len(val_imagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genrating labels using MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_csv(\"train.csv\")\n",
    "\n",
    "labels_all=[]\n",
    "for i in list(set(labels.color)):\n",
    "    for j in list(set(labels.type)):\n",
    "        labels_all.append(tuple([i,j]))\n",
    "unique_labels=set(labels_all)        \n",
    "\n",
    "unique_labels_list=list(unique_labels)\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(unique_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beige', 'black', 'blazer', 'blue', 'brown', 'dress', 'green',\n",
       "       'grey', 'hoodie', 'indigo', 'jacket', 'jeans', 'multicolor',\n",
       "       'orange', 'pink', 'purple', 'red', 'shirt & t-shirt', 'white',\n",
       "       'yellow'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data size is huge, so generators are used for consuming elements batchwise.\n",
    "nb_rows=96\n",
    "nb_cols=96\n",
    "nb_channel=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    return data/127.5-1\n",
    "\n",
    "def init_batch_data(batch_size):\n",
    "    batch_data = np.zeros((batch_size,nb_rows, nb_cols, nb_channel)) \n",
    "    batch_labels = np.zeros((batch_size,len(mlb.classes_))) # batch_labels is the one hot representation of the output\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imagePaths)\n",
    "curr_dt_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tried image aug but accuracy didnt improve\n",
    "\n",
    "# def get_random_affine():\n",
    "#     dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "#     M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "#     return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                20500     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 8,693,652\n",
      "Trainable params: 8,690,772\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def load_batch_images(batch_num, batch_size, t):\n",
    "    batch_data,batch_labels = init_batch_data(batch_size)\n",
    "    start_idx=batch_num*batch_size\n",
    "    end_idx=start_idx+batch_size\n",
    "    list_images=t[start_idx:end_idx]\n",
    "    idx=0\n",
    "    for i in list_images:\n",
    "        image = cv2.imread(i, cv2.IMREAD_COLOR)\n",
    "        if image is None or image.shape==(0,0,0):\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "            resized_norm=normalize_data(resized)\n",
    "            batch_data[idx] = resized_norm\n",
    "            labels_list=[]\n",
    "            labels_list.append(tuple([labels.loc[labels.uid==i.split(\"/\")[-1].split(\".jpg\")[0]][[\"color\",\"type\"]].values[0][0],labels.loc[labels.uid==i.split(\"/\")[-1].split(\".jpg\")[0]][[\"color\",\"type\"]].values[0][1]]))\n",
    "            batch_labels[idx]=mlb.transform(labels_list)\n",
    "            idx+=1\n",
    "        \n",
    "    return batch_data,batch_labels\n",
    "\n",
    "def generator(imagePaths,batch_size):\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(imagePaths)\n",
    "        num_batches = len(t)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            # you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield load_batch_images( batch, batch_size, t)\n",
    "            \n",
    "\n",
    "        \n",
    "        # Code for the remaining data points which are left after full batches\n",
    "        if (len(train_imagePaths) != batch_size*num_batches):\n",
    "            batch_size = len(train_imagePaths) - (batch_size*num_batches)\n",
    "            yield load_batch_images(batch, batch_size, t)\n",
    "\n",
    "\n",
    "def build(width,height,depth,classes,act=\"softmax\"):\n",
    "    model=Sequential()\n",
    "    inputShape=(height,width,depth)\n",
    "    chanDim=-1\n",
    "    if K.image_data_format()==\"channels_first\":\n",
    "        inputShape=(depth,height,width)\n",
    "        chanDim=1\n",
    "    ## conv-relu-pool\n",
    "    model.add(Conv2D(32,(3,3),padding=\"same\",input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPool2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    ##conv-relu *2 ,pool\n",
    "    model.add(Conv2D(64,(3,3),padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(Conv2D(64,(3,3),padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    ## conv-relu*2,pool\n",
    "    model.add(Conv2D(128,(3,3),padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(Conv2D(128,(3,3),padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    ## flatten\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    ##\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(act))\n",
    "\n",
    "    return model\n",
    "\n",
    "model=build(nb_rows,nb_cols,nb_channel,classes=len(mlb.classes_),act=\"sigmoid\")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "INIT_LR=1e-4\n",
    "batch_size= 32\n",
    "IMAGE_DIMS = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = Adam(learning_rate=INIT_LR) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "train_generator = generator(train_imagePaths,batch_size)\n",
    "val_generator = generator(val_imagePaths,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{accuracy:.5f}-{val_loss:.5f}-{val_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the Reducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_sequences=len(train_imagePaths)\n",
    "num_val_sequences=len(val_imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481 54\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch,validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model wieights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"models/model-00001-0.05778-0.97879-0.00000-0.99802.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "481/481 [==============================] - 950s 2s/step - loss: 0.0721 - accuracy: 0.9739 - val_loss: 5.3696e-21 - val_accuracy: 0.9976\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to model_init_2020-02-0205_10_33.338186/model-00001-0.07214-0.97390-0.00000-0.99760.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x149633b10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n",
    "#                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "481/481 [==============================] - 1502s 3s/step - loss: 0.0660 - accuracy: 0.9760 - val_loss: 0.5781 - val_accuracy: 0.9532\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57812, saving model to model_init_2020-02-0310_03_30.616889/model-00001-0.06605-0.97598-0.57812-0.95320.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15bd14450>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n",
    "#                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "481/481 [==============================] - 1174s 2s/step - loss: 0.0600 - accuracy: 0.9782 - val_loss: 2.3410e-17 - val_accuracy: 0.9980\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to model_init_2020-02-0311_19_13.708401/model-00001-0.05998-0.97818-0.00000-0.99795.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1509605d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n",
    "#                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "481/481 [==============================] - 1029s 2s/step - loss: 0.0578 - accuracy: 0.9788 - val_loss: 6.5608e-17 - val_accuracy: 0.9980\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to model_init_2020-02-0316_52_18.049499/model-00001-0.05778-0.97879-0.00000-0.99802.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14ce0d4d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n",
    "#                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "labels=pd.read_csv(\"train.csv\")\n",
    "labels_all=[]\n",
    "for i in list(set(labels.color)):\n",
    "    for j in list(set(labels.type)):\n",
    "        labels_all.append(tuple([i,j]))\n",
    "unique_labels=set(labels_all)        \n",
    "\n",
    "unique_labels_list=list(unique_labels)\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(unique_labels_list)\n",
    "\n",
    "##helper function for normalizing test data\n",
    "def normalize_data(data):\n",
    "    return data/127.5-1\n",
    "\n",
    "##helper function for getting color and type\n",
    "def get_color_type(pred_classes):\n",
    "    if pred_classes[0] in color_list:\n",
    "        color=pred_classes[0]\n",
    "        type_dress=pred_classes[1]\n",
    "    else:\n",
    "        color=pred_classes[1]\n",
    "        type_dress=pred_classes[0]\n",
    "    return color,type_dress\n",
    "\n",
    "color_list=set(labels.color)\n",
    "dress_type=set(labels.type)\n",
    "\n",
    "nb_rows=96\n",
    "nb_cols=96\n",
    "nb_channel=3\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "def predict(path_to_test_csv,imgs_directory=\"imgs/test/\"):\n",
    "    '''path_to_test_csv - test csv which will contain the test image uids\n",
    "       returns - a dataframe containing the test image uid, type and color as 3 different columns\n",
    "     '''\n",
    "    test_images=pd.read_csv(path_to_test_csv)\n",
    "    test_len=len(test_images)\n",
    "    print(\"test_images are {} in number\".format(test_len))\n",
    "    test_data = np.zeros((test_len,nb_rows, nb_cols, nb_channel)) \n",
    "    idx=0\n",
    "    uid_list=[]\n",
    "    color_list=[]\n",
    "    type_list=[]\n",
    "    for i in test_images.uid.values:  ## uid column\n",
    "        image = cv2.imread(imgs_directory+str(i)+\".jpg\", cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if image is None or image.shape==(0,0,0):\n",
    "            continue\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            #print(image.shape)\n",
    "            resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "            resized_norm=normalize_data(resized)\n",
    "            test_data[idx] = resized_norm\n",
    "            idx+=1\n",
    "            uid_list.append(str(i))\n",
    "            \n",
    "    \n",
    "    model_pred=model.predict(test_data)\n",
    "    classes=[i[-2:] for i in np.argsort(model_pred)]\n",
    "    #print(classes)\n",
    "    pred_classes=[]\n",
    "    for i in classes:\n",
    "        pred_classes.append(mlb.classes_[i])\n",
    "    \n",
    "    for i in pred_classes:\n",
    "        color,type_dress=get_color_type(i)\n",
    "        color_list.append(color)\n",
    "        type_list.append(type_dress)\n",
    "    \n",
    "    \n",
    "    all_res=[uid_list,type_list,color_list]\n",
    "    end_time=time.time()\n",
    "    print(\"Time taken for {0} test images is {1} seconds\".format(test_len,(end_time-start_time)))\n",
    "    \n",
    "    print('Predict function ends')\n",
    "    \n",
    "    return pd.DataFrame(list(zip(*all_res)),columns=[\"uid\",\"type\",\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! which python\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# import pil\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.getcwd()\n",
    "\n",
    "# train_images=len(os.listdir(\"imgs/train/\"))\n",
    "\n",
    "# shape_list=[]\n",
    "# for i in range(train_images):\n",
    "#     try:\n",
    "#         shape_list.append(cv2.imread(os.path.join(os.path.join(os.path.abspath(\"\"),\"imgs/train/\"),os.listdir(\"imgs/train/\")[i])).shape)\n",
    "#     except:\n",
    "#         shape_list.append((0,0,0))\n",
    "\n",
    "# len(set(shape_list))\n",
    "\n",
    "# max([i[0] for i in list(set(shape_list))]),max([i[1] for i in list(set(shape_list))])\n",
    "\n",
    "# min([i[0] for i in list(set(shape_list))]),min([i[1] for i in list(set(shape_list))])\n",
    "\n",
    "# min_0=[]\n",
    "# for i in list(set(shape_list)):\n",
    "#     if i[0]==0:\n",
    "#         continue\n",
    "#     min_0.append(i[0])\n",
    "# min_1=[]\n",
    "# for i in list(set(shape_list)):\n",
    "#     if i[1]==0:\n",
    "#         continue\n",
    "#     min_1.append(i[1])\n",
    "\n",
    "    \n",
    "\n",
    "# min(min_0),min(min_1)\n",
    "\n",
    "# (680-1742),(312-952),3\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# train_labels=pd.read_csv(\"train.csv\",sep=\",\")\n",
    "\n",
    "# train_labels.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "# train_labels.head()\n",
    "\n",
    "# train_labels.loc[train_labels.uid=='ac84226721']\n",
    "\n",
    "# len(set(train_labels.color)),len(set(train_labels.type))## total labels (14*6)=84\n",
    "\n",
    "# train_labels[\"final_label\"]=train_labels.apply(lambda x:x[\"color\"]+\"_\"+x[\"type\"],axis=1)\n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# pd.DataFrame(list(zip(Counter(train_labels.final_label).keys(),Counter(train_labels.final_label).values())),columns=[\"label\",\"count\"]).sort_values(\"count\",ascending=False)\n",
    "\n",
    "# img = cv2.imread(os.path.join(os.path.join(os.path.abspath(\"\"),\"imgs/train/\"),os.listdir(\"imgs/train/\")[0]))\n",
    " \n",
    "# print('Original Dimensions : ',img.shape)\n",
    "\n",
    "# import pil\n",
    "\n",
    "# width = 150\n",
    "# height = 150\n",
    "# dim = (width, height)\n",
    " \n",
    "# # resize image\n",
    "# resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "# print('Resized Dimensions : ',resized.shape)\n",
    " \n",
    "# cv2.imshow(\"Resized image\", resized)\n",
    "\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# im = Image.open(os.path.join(os.path.join(os.path.abspath(\"\"),\"imgs/train/\"),os.listdir(\"imgs/train/\")[3]))\n",
    "# im.show()\n",
    "\n",
    "# new_width  = 96\n",
    "# new_height = 96\n",
    "# im1 = im.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "# im1.show()\n",
    "\n",
    "# imagePath=os.path.join(os.path.join(os.path.abspath(\"\"),\"imgs/train/\"),os.listdir(\"imgs/train/\")[3])\n",
    "\n",
    "# imagePath\n",
    "\n",
    "# import cv2\n",
    "# image = cv2.imread(imagePath)\n",
    "# image = cv2.resize(image, (96,96))\n",
    "# cv2.imshow('ImageWindow', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "# for i in range (1,5):\n",
    "#     cv2.waitKey(1)\n",
    "\n",
    "\n",
    "# for imagePath in imagePaths:\n",
    "# # load the image, pre-process it, and store it in the data list\n",
    "#     image = cv2.imread(imagePath)\n",
    "#     try:\n",
    "#         data.append(img_to_array(cv2.resize(image, (96,96))))\n",
    "#     except:\n",
    "#         data.append(np.zeros((96,96,3)))\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open('outfile', 'wb') as fp:\n",
    "#     pickle.dump(data, fp)\n",
    "\n",
    "\n",
    "\n",
    "# batch_data,batch_labels = init_batch_data(32)\n",
    "\n",
    "# batch_data.shape,batch_labels.shape\n",
    "\n",
    "# batch_num=0\n",
    "# batch_size=32\n",
    "# start_idx=batch_num*batch_size\n",
    "# end_idx=start_idx+batch_size\n",
    "# list_images=imagePaths[start_idx:end_idx]\n",
    "\n",
    "\n",
    "# labels[labels.uid==\"1cd27c5d9f\"]\n",
    "\n",
    "# image = cv2.imread(\"imgs/train/1cd27c5d9f.jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "# resized_norm=normalize_data(resized)\n",
    "\n",
    "# model.predict(resized_norm.reshape((1,96,96,3))) #0.87,0.9\n",
    "\n",
    "# mlb.classes_\n",
    "\n",
    "# labels.type\n",
    "\n",
    "# color_list=set(labels.color)\n",
    "# dress_type=set(labels.type)\n",
    "\n",
    "\n",
    "\n",
    "# a=np.array([4,1,2,5])\n",
    "# classes=a[np.argsort(a)[-2:]]\n",
    "# pred_classes=mlb.classes_[classes]\n",
    "\n",
    "# if pred_classes[0] in color_list:\n",
    "#     color=pred_classes[0]\n",
    "#     type_dress=pred_classes[1]\n",
    "# else:\n",
    "#     color=pred_classes[1]\n",
    "#     type_dress=pred_classes[0]\n",
    "    \n",
    "\n",
    "# color,type_dress\n",
    "\n",
    "# model.predict(resized_norm.reshape((1,96,96,3)))\n",
    "\n",
    "# sample=pd.DataFrame([\"1cd27c5d9f\",\"1cbc8001d4\"],columns=[\"uid\"])\n",
    "\n",
    "\n",
    "# image = cv2.imread(\"imgs/train/\"+\"560a89382f\"+\".jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "# \"imgs/train/\"+\"560a89382f\"+\".jpg\"\n",
    "\n",
    "# set(sample.uid.values).intersection(set(labels[(labels.type==\"hoodie\") & (labels.color==\"green\")].uid.values))\n",
    "\n",
    "# np.where(sample.uid.values=='90568d2fec')\n",
    "\n",
    "# pred_classes[pred_classes.uid=='598a4be155']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp '5cf821ba3f.jpg'| 'd8a3543a7f.jpg'| 'e0810f4cfc.jpg'| '244a7d7408.jpg'| '08781caf21.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(lambda x:x+'.jpg',list(pd.read_csv(\"sample_df.csv\").uid.values))),5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.6504902e-10 1.0546199e-06 4.1343912e-05 2.2967513e-06 1.4768435e-09\n",
      "  6.1506427e-08 1.2623284e-11 3.2892305e-04 1.4043157e-07 1.0491231e-07\n",
      "  8.6565800e-03 2.3046483e-03 2.5819789e-09 1.9483146e-09 2.3152943e-11\n",
      "  3.5753540e-09 2.8643354e-10 3.6211629e-04 2.4563568e-02 1.8434921e-08]]\n",
      "['jeans' 'jacket' 'white']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil.birajdar/Downloads/upgrad/ugam/env/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"wj.jpeg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "resized_norm=normalize_data(resized)\n",
    "\n",
    "model_pred=model.predict(resized_norm.reshape((1,96,96,3)))\n",
    "print(model_pred)\n",
    "classes=[i[-3:] for i in np.argsort(model_pred)]\n",
    "pred_classes=mlb.classes_[classes]\n",
    "print(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14194.901960784313"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_norm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14194.901960784313"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_norm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list=[]\n",
    "for i in val_imagePaths:\n",
    "    image = cv2.imread(i, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "    resized_norm=normalize_data(resized)\n",
    "    sum_list.append(resized_norm.sum())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 35., 218., 291., 251., 171., 172., 185., 196., 157.,  34.]),\n",
       " array([-9955.4745098 , -6594.34823529, -3233.22196078,   127.90431373,\n",
       "         3489.03058824,  6850.15686275, 10211.28313725, 13572.40941176,\n",
       "        16933.53568627, 20294.66196078, 23655.78823529]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-9e978b77739e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
